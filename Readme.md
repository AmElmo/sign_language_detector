# ğŸ¤” Problem

70 millions people in the world are considered functionally deaf.
The vast majority of health practicioners do not speak sign languages. Communications with deaf patients is a struggle.


# ğŸ’¡ Solution

Sign language detector for medical practitioner to help them understand symptoms from hearing-impaired patients.
Our model is trained on the most common symptoms that doctors and nurses may face on a daily basis.

We use American Sign Language (ASL) for this project but the code can be reused to train on any form of sign language.


# ğŸ¤– Stack overview


Open CV: to work easily with our webcam to generat our dataset
Mediapipe: Google library to extract keypoints from a human body in real time
Tensorflow: for building and training our models
Tensorflow JS: for converting to layer model or graph model and use in a JavaScript application
Numpy: classic for dealing with data extraction and pre-processing


# ğŸªœ Project steps

## 1. ğŸ’½ Data collection

We first considered a large-scale dataset for Word-Level American Sign Language: https://github.com/dxli94/WLASL - but since we used a reduced scope of 9 words to translate for this project, we generated our own dataset with our own videos.

We have collected 30 videos of 30 frames for each word.


## 1. ğŸ’½ Data preprocessing

We've saved each frame of each video as a numpy array of size

ğŸ’¡ Tip: if using Google Colab for training, you must zip and unzip the data before in order to allow for 10x faster processing with Numpy.


## 2. â³ Training an LSTM neural network

## 2. ğŸ§  Training a CNN + LSTM neural network



## 3. ğŸ“½ Evaluation and test in real time

We logged metrics from our model and displayed them in Tensorboard.

## 3. ğŸš² Lifecycle setup



## 4. ğŸŒŸ Convert model to Tensorflow JS

Depending on the type of model built, we converted our .h5 Keras model to a layers model or graph model.

LSTM ğŸ‘‰ layers model

CNN + LSTM ğŸ‘‰ graph model
